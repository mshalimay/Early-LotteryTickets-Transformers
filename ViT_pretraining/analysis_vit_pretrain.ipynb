{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import argparse\n",
    "import pymannkendall as mk\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import os\n",
    "\n",
    "save_dir = 'images/vit/'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globals and function to load/plot slimming coeficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    1: './vit_outputs/202404_28_1417', # finetune google's model on imnet1k\n",
    "    2: './vit_outputs/vit_c10_aa_ls_202404_24_1449_202404_24_1449' # pretraining on c10\n",
    "}\n",
    "self_coefs_file = 'self_slimming_coef_records.npy'   \n",
    "inter_coefs_file = 'inter_slimming_coef_records.npy'\n",
    "\n",
    "\n",
    "def load_and_plot(experiment, self=True, inter=True, plot=True, save=False):\n",
    "    os.makedirs(f'{save_dir}/self_attention_coefs', exist_ok=True)\n",
    "    experiment_path = experiments[experiment]\n",
    "    \n",
    "    with open(f'{experiment_path}/args.json', 'r') as f:\n",
    "        args_dict = json.load(f)\n",
    "    # Convert dictionary to an argparse.Namespace object\n",
    "    args = argparse.Namespace(**args_dict)\n",
    "\n",
    "    self_coefs, inter_coefs = None, None\n",
    "    \n",
    "    if self:\n",
    "        self_coefs = np.load(f'{experiment_path}/{self_coefs_file}')\n",
    "\n",
    "    if inter:\n",
    "        inter_coefs = np.load(f'{experiment_path}/{inter_coefs_file}')\n",
    "\n",
    "    # print(f\"slim_before: {args.slim_before}, soft_by_one: {args.soft_by_one}\")\n",
    "    # epoch_0 = self_coefs.shape[1] // 2\n",
    "    if plot and self:\n",
    "        for layer in range(6):\n",
    "            fig, axs = plt.subplots(4, 3, figsize=(15, 15))\n",
    "            for i in range(4):\n",
    "                for j in range(3):\n",
    "                    idx = i * 3 + j\n",
    "                    axs[i, j].plot(self_coefs[layer,:,idx], color='black')\n",
    "                    # space between subplots\n",
    "                    axs[i, j].set_title(f'Head {idx + 1}')\n",
    "                    axs[i, j].set_xlabel('Steps')\n",
    "                    axs[i, j].set_ylabel('Slimming Coefficient')\n",
    "\n",
    "                    # vertical line at end of epoch 0\n",
    "                    # axs[i, j].axvline(x=epoch_0, color='r', linestyle='--', label='End Epoch 0')\n",
    "\n",
    "            plt.suptitle(f'Slimming Coefficients - Layer {layer}', fontsize=16)  # Adjust y position of the title\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, .98])  # Adjusted top margin\n",
    "            plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "            if save:\n",
    "                plt.savefig(f'{save_dir}/self_attention_coefs/layer_{layer}.png')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    if plot and inter:\n",
    "        fig, axs = plt.subplots(4, 3, figsize=(15, 20))  # create a 4x3 grid of subplots\n",
    "        axs = axs.flatten() \n",
    "        for layer in range(inter_coefs.shape[0]):\n",
    "            ax = axs[layer] \n",
    "\n",
    "            ax.plot(inter_coefs[layer,:,:])\n",
    "            ax.set_title(f'Layer {layer}')\n",
    "            ax.set_xlabel('Steps')\n",
    "            ax.set_ylabel('Slimming Coefficient')\n",
    "\n",
    "\n",
    "        plt.tight_layout()  \n",
    "        if save:\n",
    "            plt.savefig(f'{save_dir}/inter_attention_coefs/layer_{layer}.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    return self_coefs, inter_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - Multihead Self-Attention Slimming Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_coefs, inter_coefs, = load_and_plot(2, self=True, inter=False, plot=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - Traditional EarlyBird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(slimming_coefs, pruning_ratio=0.3, quantile_axis=None, method='layerwise'):\n",
    "    if quantile_axis is None:\n",
    "        quantile_axis = -1 if method == 'layerwise' else None\n",
    "    threshold = np.quantile(slimming_coefs, pruning_ratio, axis=quantile_axis, keepdims=True)\n",
    "    return slimming_coefs > threshold\n",
    "\n",
    "def hamming_distance(mask1, mask2):\n",
    "    return 1 - float(np.sum(mask1==mask2)) / len(mask2.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_ratio=0.3\n",
    "layers_masks = np.stack([compute_mask(self_coefs[:,step,:], method='layerwise', pruning_ratio=pruning_ratio) for step in range(self_coefs.shape[1])], axis=1)\n",
    "distances = np.array([hamming_distance(layers_masks[:,i,:], layers_masks[:,i+1,:]) for i in range(1, layers_masks.shape[1]-1)])\n",
    "\n",
    "plt.plot(distances)  # add border to markers\n",
    "plt.title(f'Hamming Distances between consecutive masks, pruning={int(pruning_ratio*100)}%', fontsize=10)  # set title\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Hamming distance')\n",
    "plt.savefig(f\"{save_dir}/hamming_distances_p={pruning_ratio}.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# histogram of hamming distances\n",
    "plt.hist(distances, bins=20, range=(0, .05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class EarlyBird():\n",
    "    def __init__(self, percent, epoch_keep=5, masks=None, thresh=0.1):\n",
    "        self.percent = percent\n",
    "        self.epoch_keep = epoch_keep\n",
    "        self.masks = []\n",
    "        self.dists = [1 for i in range(1, self.epoch_keep)]\n",
    "        self.count=0\n",
    "        self.masks_all = masks\n",
    "        self.thresh=thresh\n",
    "        \n",
    "    def put(self, mask):\n",
    "        if len(self.masks) < self.epoch_keep:\n",
    "            self.masks.append(mask)\n",
    "        else:\n",
    "            self.masks.pop(0)\n",
    "            self.masks.append(mask)\n",
    "\n",
    "    def cal_dist(self):\n",
    "        if len(self.masks) == self.epoch_keep:\n",
    "            for i in range(len(self.masks)-1):\n",
    "                mask_i = self.masks[-1]\n",
    "                mask_j = self.masks[i]\n",
    "                self.dists[i] = 1 - float(np.sum(mask_i==mask_j)) / len(mask_j.flatten())\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def early_bird_emerge(self, model):\n",
    "        # mask = self.pruning(model, self.percent)\n",
    "        mask = self.masks_all[:,self.count,:]\n",
    "        self.count+=1\n",
    "        self.put(mask)\n",
    "        flag = self.cal_dist()\n",
    "        if flag == True:\n",
    "            for i in range(len(self.dists)):\n",
    "                if self.dists[i] > self.thresh:\n",
    "                    return False\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_early_bird(slimming_coefs, pruning_ratio, epoch_keep=5, \n",
    "                    thresh=0.1, method='layerwise', verbose=True, step = 1):\n",
    "    layers_masks = np.stack([compute_mask(slimming_coefs[:,step,:], pruning_ratio=pruning_ratio) for step in range(slimming_coefs.shape[1])], axis=1)\n",
    "    quantile_axis = -1\n",
    "\n",
    "    quantile_axis = -1 if method == 'layerwise' else None\n",
    "    threshold = np.quantile(slimming_coefs, pruning_ratio, axis=quantile_axis, keepdims=True)\n",
    "    layers_masks = slimming_coefs > threshold\n",
    "\n",
    "    eb = EarlyBird(pruning_ratio, epoch_keep=epoch_keep, masks=layers_masks, thresh=thresh)\n",
    "\n",
    "    for i in range(0, layers_masks.shape[1], step):\n",
    "        if eb.early_bird_emerge(None):\n",
    "            if verbose:\n",
    "                print(f'Early bird emerges at step {i}')\n",
    "            break\n",
    "\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw early birds for self attention heads\n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "pruning_ratios=[0.3,  0.8]\n",
    "cached_masks = [5, 50, 100, 500, 1000, 2000]\n",
    "thresh = [0.1, 0.05, 0.01, 0]\n",
    "steps = [10, 25, 50]\n",
    "combinations = itertools.product(pruning_ratios, cached_masks, thresh, steps)\n",
    "\n",
    "epochs_drawn = Parallel(n_jobs=4)(delayed(draw_early_bird)(self_coefs, pruning_ratio, epoch_keep=cached_mask, thresh=th, method='layerwise', verbose=False, step=step) \n",
    "                                  for pruning_ratio, cached_mask, th, step in combinations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Recreate the combinations as a list\n",
    "combinations_list = list(itertools.product(pruning_ratios, cached_masks, thresh, steps))\n",
    "\n",
    "# Create a DataFrame from the parameters and epochs drawn\n",
    "df = pd.DataFrame(combinations_list, columns=['Pruning Ratio', 'Steps Keep', 'Threshold', 'Steps'])\n",
    "\n",
    "# Add the epochs_drawn data assuming it's the same length as combinations_list\n",
    "df['Epochs Drawn'] = epochs_drawn\n",
    "\n",
    "\n",
    "# save to csv file\n",
    "df.to_csv(f'{save_dir}/early_birds_selfslimming_steps.csv')\n",
    "\n",
    "# # Filter the DataFrame for the specific pruning ratio\n",
    "# pivot_table = df[df['Pruning Ratio'] == .2]\n",
    "\n",
    "# # Pivot the table\n",
    "# pivot_table = pivot_table.pivot(columns='Threshold', index='Steps Keep', values='Epochs Drawn')\n",
    "# pivot_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity in MSA Slimming Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize sliding slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head=0\n",
    "layer=0\n",
    "end = self_coefs.shape[1]\n",
    "\n",
    "start = 0\n",
    "step = 500\n",
    "window = 500\n",
    "# window_smooth=100\n",
    "t0 = start\n",
    "t1 = start + (window-1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Sliding window regressions - MSA slimming coefs. layer 0, head 0\")\n",
    "plt.plot(self_coefs[layer, :, head])\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Slimming Coefficients')\n",
    "\n",
    "slopes=[]\n",
    "\n",
    "while t1 < end:\n",
    "    series = self_coefs[layer, t0:t1, head]\n",
    "    # series = np.convolve(series, np.ones(window_smooth)/window_smooth, mode='valid')\n",
    "    t = np.arange(t0, t1)\n",
    "    fitted = sm.OLS(series, sm.add_constant(t)).fit()\n",
    "\n",
    "    # plot the support of the fitted line\n",
    "    intercept, slope = fitted.params\n",
    "\n",
    "    slopes.append(slope)\n",
    "    line = slope * t + intercept\n",
    "    plt.plot(t, line, color='red')\n",
    "    \n",
    "    t0+=step\n",
    "    t1+=step\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different methods to identify change of trends / stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head=0\n",
    "layer=0\n",
    "\n",
    "smooth=False\n",
    "window_smooth=100\n",
    "\n",
    "series = self_coefs[layer, :, head]\n",
    "end = series.shape[1]\n",
    "print(end)\n",
    "start = 0\n",
    "step = 100\n",
    "window = 500\n",
    "\n",
    "tests = []\n",
    "intervals = []\n",
    "\n",
    "\n",
    "t0 = start\n",
    "t1 = start + (window-1)\n",
    "\n",
    "methods = ['mk', 'adf', 'db', 'autoreg', 'lin_reg']\n",
    "results = {method: [] for method in methods}\n",
    "\n",
    "while t1 < end:\n",
    "    intervals.append((t0, t1))\n",
    "    series = series[layer, t0:t1, head]\n",
    "    # if smooth: series = np.convolve(series, np.ones(window_smooth)/window_smooth, mode='valid')\n",
    "    \n",
    "    results['mk'].append(mk.original_test(series[layer, t0:t1, head]))\n",
    "    results['adf'].append(sm.tsa.adfuller(series[layer, t0:t1, head], autolag='AIC', regression='c'))\n",
    "    results['db'].append(durbin_watson(series[layer, t0:t1, head]))\n",
    "\n",
    "    # run an AR model\n",
    "    # test_result = AutoReg(series, lags=1, old_names=False, trend='ct').fit()\n",
    "    # test_result = AutoReg(endog=series, lags=0, exog=exogs, trend='n', old_names=False).fit()\n",
    "\n",
    "    # linear regressio against trend including constant\n",
    "    t = np.arange(len(series))\n",
    "    results['lin_reg'].append(sm.OLS(series, sm.add_constant(t)).fit())\n",
    "    \n",
    "    t0 += step\n",
    "    t1 = t0 + (window-1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar1 = [test.params[1] for test in tests]\n",
    "# plt.plot(ar1)\n",
    "# plt.show()\n",
    "# plot the p-values\n",
    "p_values = [reg.pvalues[1]*1000 for reg in results['lin_reg']]\n",
    "plt.plot(p_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mk p-values\n",
    "p_values = [results['mk'][i].p for i in range(len(intervals))]\n",
    "plt.plot(p_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p < 0.05 means the series IS stationary (reject unit root hypothesis)\n",
    "p_values = [results['adf'][i][1] for i in range(len(intervals))]\n",
    "# plt.plot(p_values)\n",
    "# plt.show()\n",
    "\n",
    "# plot p<0.05 or not\n",
    "binary_p = [results['adf'][i][1] < 0.05 for i in range(len(intervals))]\n",
    "plt.plot(binary_p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of MSA nuclear norm masks and Stationarity metris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def norm(mask, ord='nuc'):\n",
    "    return np.linalg.norm(mask, ord=ord)\n",
    "\n",
    "def moving_average(y, window_size):\n",
    "    return np.convolve(y, np.ones(window_size)/ window_size, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_ratio=0.2\n",
    "layers_masks = np.stack([compute_mask(self_coefs[:,step,:], pruning_ratio=pruning_ratio) for step in range(self_coefs.shape[1])], axis=1)\n",
    "\n",
    "norms = [norm(layers_masks[:,i,:], 'nuc') for i in range(1, layers_masks.shape[1]-1)]\n",
    "\n",
    "plt.plot(norms, label='Nuclear Norm of MSA masks')\n",
    "plt.plot(moving_average(norms, 500), label='Moving Average 500 steps', color='black')\n",
    "plt.legend()\n",
    "plt.xlabel('Training Steps')\n",
    "plt.plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Chart Smoothed norm for different intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time  # Import time for sleep functionality\n",
    "series =  norms\n",
    "end = len(series)\n",
    "start = 0\n",
    "window_smooth=200\n",
    "step = 100\n",
    "window = 300\n",
    "\n",
    "tests = []\n",
    "intervals = []\n",
    "\n",
    "t0 = start\n",
    "t1 = t0 + (window) + (window_smooth-1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "frame = 0\n",
    "while t1<end:\n",
    "    y = series[t0:t1]\n",
    "    y = np.convolve(y, np.ones(window_smooth) / window_smooth, mode='valid')\n",
    "\n",
    "    ax.clear()  # Clear the axis to draw a new plot\n",
    "    ax.plot(y)  # Plot the new segment\n",
    "    ax.set_title(f\"Plot from index {t0} to {t1}\")\n",
    "    ax.set_xlabel('Index within Window')\n",
    "    ax.set_ylabel('Smoothed Value')\n",
    "\n",
    "    display(fig)    # Display the figure\n",
    "    clear_output(wait=True)  # Clear the output to make room for the next plot\n",
    "    time.sleep(1)  # Pause for half a second before continuing to the next frame\n",
    "\n",
    "    frame += 1  # Increment frame counter\n",
    "    t0 += step\n",
    "    t1 = t1 + step\n",
    "    \n",
    "\n",
    "plt.close(fig)  # Close the figure to clean up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize sliding slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "step = 100\n",
    "window_smooth=200\n",
    "step = 200\n",
    "window = 300\n",
    "end = len(series)\n",
    "\n",
    "\n",
    "series = np.convolve(series, np.ones(window_smooth)/window_smooth, mode='valid')\n",
    "\n",
    "\n",
    "t0 = start\n",
    "t1 = t0 + (window) + (window_smooth-1)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(series)\n",
    "\n",
    "slopes=[]\n",
    "while t1 < end:\n",
    "    y = series[t0:t1]\n",
    "    t = np.arange(t0, t1)\n",
    "    fitted = sm.OLS(y, sm.add_constant(t)).fit()\n",
    "\n",
    "    # plot the support of the fitted line\n",
    "    intercept, slope = fitted.params\n",
    "\n",
    "    slopes.append(slope)\n",
    "    line = slope * t + intercept\n",
    "    plt.plot(t, line, color='red')\n",
    "    \n",
    "    t0+=step\n",
    "    t1+=step\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early-bird drawing - MSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stationarity_measures(series, start=0, window_smooth=200, step=100, window=300):\n",
    "    end = len(series)\n",
    "    t0 = start\n",
    "    t1 = t0 + (window) + (window_smooth-1)\n",
    "    methods = ['mk', 'adf', 'db', 'lin_reg']\n",
    "    results = {method: [] for method in methods}\n",
    "    steps = []\n",
    "    while t1 < end:\n",
    "        y = series[t0:t1]\n",
    "        if window_smooth > 0:\n",
    "            y = np.convolve(y, np.ones(window_smooth)/window_smooth, mode='valid')\n",
    "        results['mk'].append(mk.original_test(y))\n",
    "        try:\n",
    "            results['adf'].append(sm.tsa.adfuller(y, autolag='AIC', regression='c')[1])\n",
    "        except:\n",
    "            results['adf'].append(0)\n",
    "        results['db'].append(durbin_watson(y))\n",
    "        t = np.arange(len(y))\n",
    "        results['lin_reg'].append(sm.OLS(y, sm.add_constant(t)).fit())\n",
    "        t0 += step\n",
    "        t1 = t0 + (window-1)\n",
    "        steps.append(t1)\n",
    "    return results, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_p_values(steps, p_values, title, threshold=0.1, max_labels=8):\n",
    "    plt.plot(steps, p_values, color='black')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Training Steps')\n",
    "    \n",
    "    # horizontal line at threshold\n",
    "    if threshold > 0:\n",
    "        plt.axhline(y=threshold, color='r', linestyle='--', label=str(threshold))\n",
    "\n",
    "    # label the points above threshold with the corresponding steps\n",
    "    above_threshold = p_values > threshold\n",
    "    nlabels=0\n",
    "    for step, p_value in zip(steps[above_threshold], p_values[above_threshold]):\n",
    "        if nlabels >= max_labels:\n",
    "            break\n",
    "        plt.text(step, p_value, str(step), color='red', fontsize=6)\n",
    "        nlabels += 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # get indexes of p-values greater than threshold\n",
    "    p_greater_than = np.where(p_values > threshold)[0]\n",
    "    print(steps[p_greater_than])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_ratio=0.3\n",
    "layers_masks = np.stack([compute_mask(self_coefs[:,step,:], pruning_ratio=pruning_ratio) for step in range(self_coefs.shape[1])], axis=1)\n",
    "norms = [norm(layers_masks[:,i,:], 'nuc') for i in range(1, layers_masks.shape[1]-1)]\n",
    "\n",
    "\n",
    "self_stats, steps = stationarity_measures(norms, start=0, window_smooth=200, step=100, window=300)\n",
    "steps=np.array(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = np.array([reg.pvalues[1] for reg in self_stats['lin_reg']])\n",
    "\n",
    "plot_p_values(steps, p_values, 'EB Detection - Linear Regression p-values', threshold=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mk p-values\n",
    "p_values = np.array([self_stats['mk'][i].p for i in range(len(steps))])\n",
    "\n",
    "plot_p_values(steps, p_values, 'EB Detection - MK test p values', threshold=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# p < 0.05 means the series IS stationary (reject unit root hypothesis)\n",
    "p_values = np.array([self_stats['adf'][i] for i in range(len(steps))])\n",
    "# plt.plot(p_values)\n",
    "# plt.show()\n",
    "\n",
    "# plot p<0.05 or not\n",
    "binary_p = np.array([self_stats['adf'][i] < 0.1 for i in range(len(steps))])\n",
    "\n",
    "plot_p_values(steps, binary_p, 'EB Detection - ADF test', threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - Intermediate MLP slimming coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamming Distances are much more stable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, inter_coefs = load_and_plot(1, self=False, inter=True, plot=False, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_ratio=0.3\n",
    "# similar results for layerwise or global pruning\n",
    "layers_masks = np.stack([compute_mask(inter_coefs[:,step,:], method='global', pruning_ratio=pruning_ratio) for step in range(self_coefs.shape[1])], axis=1)\n",
    "distances = np.array([hamming_distance(layers_masks[:,i,:], layers_masks[:,i+1,:]) for i in range(1, layers_masks.shape[1]-1)])\n",
    "plt.plot(distances[2:])  # remove first initial ones for better plot\n",
    "plt.title(f'Hamming Distances between consecutive masks, pruning={int(pruning_ratio*100)}%, global', fontsize=10)  # set title\n",
    "plt.xlabel('Training Steps')\n",
    "# plt.ylabel('Hamming distance')\n",
    "plt.savefig(f\"{save_dir}/hamming_distances_p={pruning_ratio}.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_ratio = 0.3\n",
    "masks_inter = np.stack([compute_mask(inter_coefs[:,step,:], method='global', pruning_ratio=pruning_ratio) for step in range(inter_coefs.shape[1])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms_inter = [norm(masks_inter[:,i,:], 'nuc') for i in range(1, masks_inter.shape[1]-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_smooth=300\n",
    "norms_smoothed = np.convolve(norms_inter, np.ones(window_smooth)/window_smooth, mode='valid')\n",
    "plt.plot(norms_inter)\n",
    "plt.plot(norms_smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_stats, steps = stationarity_measures(norms_inter, start=0, window_smooth=0, step=100, window=300)\n",
    "steps=np.array(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = np.array([reg.pvalues[1] for reg in inter_stats['lin_reg']])\n",
    "\n",
    "plot_p_values(steps, p_values, 'EB Detection - FF blocks - Linear Regression p-values', threshold=.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = np.array([inter_stats['mk'][i].p for i in range(len(steps))])\n",
    "\n",
    "plot_p_values(steps, p_values, 'EB Detection - FF blocks - MK test p values', threshold=.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = np.array([inter_stats['adf'][i] for i in range(len(steps))])\n",
    "\n",
    "p_binary = np.array([p < 0.05 for p in p_values])\n",
    "\n",
    "plot_p_values(steps, p_binary, 'EB Detection - FF blocks - ADF test', threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: markov-switching model\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "\n",
    "head = 0\n",
    "layer = 0\n",
    "\n",
    "end = self_coefs.shape[1]\n",
    "\n",
    "start = 0\n",
    "step = 100\n",
    "window = 500\n",
    "\n",
    "# Initialize the storage for probabilities\n",
    "transition_probs = []\n",
    "intervals = []\n",
    "\n",
    "# Define initial probabilities if known, otherwise, you can start with uniform probabilities\n",
    "initial_probabilities = np.array([0.5, 0.5])\n",
    "\n",
    "# Process each window of data\n",
    "t0 = start\n",
    "t1 = start + (window-1)\n",
    "\n",
    "while t1 < end:\n",
    "    intervals.append((t0, t1))\n",
    "    series = self_coefs[layer, t0:t1, head].reshape(-1, 1)\n",
    "\n",
    "    # Create and fit the HMM\n",
    "    model = hmm.GaussianHMM(n_components=2, covariance_type=\"diag\", n_iter=100)\n",
    "    # model.startprob_ = initial_probabilities\n",
    "    model.fit(series)\n",
    "\n",
    "    if model.monitor_.converged:\n",
    "        transmat = model.transmat_\n",
    "        covars = model.covars_ # np.diag(model.covars_)\n",
    "\n",
    "        # Identify states based on variance or other criteria\n",
    "        state_constant = np.argmin(covars)\n",
    "        state_trend = np.argmax(covars)  \n",
    "\n",
    "        # Probability of transitioning from trend to constant\n",
    "        prob_trend_to_constant = transmat[state_trend, state_constant]\n",
    "        transition_probs.append(1-prob_trend_to_constant)\n",
    "        \n",
    "    # Move to the next window\n",
    "    t0 += step\n",
    "    t1 = t0 + (window-1)\n",
    "\n",
    "plt.plot(transition_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian_changepoint_detection.priors import const_prior\n",
    "from bayesian_changepoint_detection.bayesian_models import offline_changepoint_detection\n",
    "import bayesian_changepoint_detection.offline_likelihoods as offline_ll\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "head = 0\n",
    "layer = 0\n",
    "\n",
    "end = self_coefs.shape[1]\n",
    "\n",
    "start = 0\n",
    "step = 100\n",
    "window = 500\n",
    "\n",
    "# Initialize the storage for probabilities\n",
    "transition_probs = []\n",
    "\n",
    "# Process each window of data\n",
    "t0 = start\n",
    "t1 = start + (window-1)\n",
    "while t1 < end:\n",
    "    series = self_coefs[layer, t0:t1, head].reshape(-1, 1)\n",
    "    prior_function = partial(const_prior, p=1/(len(series) + 1))\n",
    "    \n",
    "    # Create and fit the HMM\n",
    "    Q, P, Pcp = offline_changepoint_detection(series, prior_function, offline_ll.StudentT(), truncate=-40)\n",
    "    transition_probs.append(np.exp(Pcp).sum())\n",
    "    t0 += step\n",
    "    t1 = t0 + (window-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import breaks_cusumolsresid\n",
    "\n",
    "# Assuming 'y' is your dependent variable and 'X' is a matrix of regressors\n",
    "X = sm.add_constant(X)  # Add an intercept term to the model\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Run CUSUM test on residuals\n",
    "test_stat, critical_values, pvalue = breaks_cusumolsresid(results.resid)\n",
    "print(\"CUSUM test statistic:\", test_stat)\n",
    "print(\"p-value:\", pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement cusum changepoint detection for dy\n",
    "\n",
    "dy = np.diff(self_coefs[layer, :, head])\n",
    "plt.plot(dy)\n",
    "\n",
    "\n",
    "def cusum_for_stability(dy, threshold=5):\n",
    "    S_pos, S_neg = 0, 0\n",
    "    stable_points = []\n",
    "\n",
    "    for i, change in enumerate(dy):\n",
    "        if change > 0:\n",
    "            S_pos += change\n",
    "            S_neg = 0  # reset negative CUSUM\n",
    "        elif change < 0:\n",
    "            S_neg += abs(change)\n",
    "            S_pos = 0  # reset positive CUSUM\n",
    "        else:\n",
    "            # When dy is zero, check for stability\n",
    "            if S_pos < threshold and S_neg < threshold:\n",
    "                stable_points.append(i)\n",
    "\n",
    "        # Reset if thresholds are exceeded\n",
    "        if S_pos > threshold:\n",
    "            S_pos = 0\n",
    "        if S_neg > threshold:\n",
    "            S_neg = 0\n",
    "\n",
    "    return stable_points\n",
    "\n",
    "\n",
    "\n",
    "stable_points = cusum_for_stability(dy)\n",
    "print(\"Stable points detected at indices:\", stable_points)\n",
    "\n",
    "plt.plot(dy, label='dy')\n",
    "plt.scatter(stable_points, dy[stable_points], c='red', label='Stable Points')\n",
    "plt.axhline(0, color='gray', lw=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earlybert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
